{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(\"./models/cldm_v15_unicontrol_v11.yaml\").cpu()\n",
    "model.load_state_dict(load_state_dict(\"./ckpts/unicontrol_v1.1.ckpt\", location='cpu'), strict=False) #, strict=False\n",
    "tokenizer = model.cond_stage_model.tokenizer\n",
    "placeholder_tokens = \"<target-hand>\"\n",
    "placeholder_token_ids = tokenizer.convert_tokens_to_ids(placeholder_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from packaging import version\n",
    "from torchvision import transforms\n",
    "import PIL\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from cldm.model import create_model, load_state_dict\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "if version.parse(version.parse(PIL.__version__).base_version) >= version.parse(\"9.1.0\"):\n",
    "    PIL_INTERPOLATION = {\n",
    "        \"linear\": PIL.Image.Resampling.BILINEAR,\n",
    "        \"bilinear\": PIL.Image.Resampling.BILINEAR,\n",
    "        \"bicubic\": PIL.Image.Resampling.BICUBIC,\n",
    "        \"lanczos\": PIL.Image.Resampling.LANCZOS,\n",
    "        \"nearest\": PIL.Image.Resampling.NEAREST,\n",
    "    }\n",
    "else:\n",
    "    PIL_INTERPOLATION = {\n",
    "        \"linear\": PIL.Image.LINEAR,\n",
    "        \"bilinear\": PIL.Image.BILINEAR,\n",
    "        \"bicubic\": PIL.Image.BICUBIC,\n",
    "        \"lanczos\": PIL.Image.LANCZOS,\n",
    "        \"nearest\": PIL.Image.NEAREST,\n",
    "    }\n",
    "    \n",
    "imagenet_templates_small = [\n",
    "    \"a photo of a {}\",\n",
    "    \"a rendering of a {}\",\n",
    "    \"a cropped photo of the {}\",\n",
    "    \"the photo of a {}\",\n",
    "    \"a photo of a clean {}\",\n",
    "    \"a photo of a dirty {}\",\n",
    "    \"a dark photo of the {}\",\n",
    "    \"a photo of my {}\",\n",
    "    \"a photo of the cool {}\",\n",
    "    \"a close-up photo of a {}\",\n",
    "    \"a bright photo of the {}\",\n",
    "    \"a cropped photo of a {}\",\n",
    "    \"a photo of the {}\",\n",
    "    \"a good photo of the {}\",\n",
    "    \"a photo of one {}\",\n",
    "    \"a close-up photo of the {}\",\n",
    "    \"a rendition of the {}\",\n",
    "    \"a photo of the clean {}\",\n",
    "    \"a rendition of a {}\",\n",
    "    \"a photo of a nice {}\",\n",
    "    \"a good photo of a {}\",\n",
    "    \"a photo of the nice {}\",\n",
    "    \"a photo of the small {}\",\n",
    "    \"a photo of the weird {}\",\n",
    "    \"a photo of the large {}\",\n",
    "    \"a photo of a cool {}\",\n",
    "    \"a photo of a small {}\",\n",
    "]\n",
    "\n",
    "imagenet_style_templates_small = [\n",
    "    \"a painting in the style of {}\",\n",
    "    \"a rendering in the style of {}\",\n",
    "    \"a cropped painting in the style of {}\",\n",
    "    \"the painting in the style of {}\",\n",
    "    \"a clean painting in the style of {}\",\n",
    "    \"a dirty painting in the style of {}\",\n",
    "    \"a dark painting in the style of {}\",\n",
    "    \"a picture in the style of {}\",\n",
    "    \"a cool painting in the style of {}\",\n",
    "    \"a close-up painting in the style of {}\",\n",
    "    \"a bright painting in the style of {}\",\n",
    "    \"a cropped painting in the style of {}\",\n",
    "    \"a good painting in the style of {}\",\n",
    "    \"a close-up painting in the style of {}\",\n",
    "    \"a rendition in the style of {}\",\n",
    "    \"a nice painting in the style of {}\",\n",
    "    \"a small painting in the style of {}\",\n",
    "    \"a weird painting in the style of {}\",\n",
    "    \"a large painting in the style of {}\",\n",
    "]\n",
    "\n",
    "class TextualInversionDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_root,\n",
    "        tokenizer,\n",
    "        learnable_property=\"object\",  # [object, style]\n",
    "        size=512,\n",
    "        repeats=100,\n",
    "        interpolation=\"bicubic\",\n",
    "        flip_p=0.5,\n",
    "        set=\"train\",\n",
    "        placeholder_token=\"*\",\n",
    "        center_crop=False,\n",
    "    ):\n",
    "        self.data_root = data_root\n",
    "        self.tokenizer = tokenizer\n",
    "        self.learnable_property = learnable_property\n",
    "        self.size = size\n",
    "        self.placeholder_token = placeholder_token\n",
    "        self.center_crop = center_crop\n",
    "        self.flip_p = flip_p\n",
    "\n",
    "        self.image_paths = [os.path.join(self.data_root, file_path) for file_path in os.listdir(self.data_root)]\n",
    "\n",
    "        self.num_images = len(self.image_paths)\n",
    "        self._length = self.num_images\n",
    "\n",
    "        if set == \"train\":\n",
    "            self._length = self.num_images * repeats\n",
    "\n",
    "        self.interpolation = {\n",
    "            \"linear\": PIL_INTERPOLATION[\"linear\"],\n",
    "            \"bilinear\": PIL_INTERPOLATION[\"bilinear\"],\n",
    "            \"bicubic\": PIL_INTERPOLATION[\"bicubic\"],\n",
    "            \"lanczos\": PIL_INTERPOLATION[\"lanczos\"],\n",
    "        }[interpolation]\n",
    "\n",
    "        self.templates = imagenet_style_templates_small if learnable_property == \"style\" else imagenet_templates_small\n",
    "        self.flip_transform = transforms.RandomHorizontalFlip(p=self.flip_p)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._length\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        example = {}\n",
    "        image = Image.open(self.image_paths[i % self.num_images])\n",
    "\n",
    "        if not image.mode == \"RGB\":\n",
    "            image = image.convert(\"RGB\")\n",
    "\n",
    "        placeholder_string = self.placeholder_token\n",
    "        text = random.choice(self.templates).format(placeholder_string)\n",
    "\n",
    "        example[\"input_ids\"] = self.tokenizer(\n",
    "            text,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.tokenizer.model_max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        ).input_ids[0]\n",
    "\n",
    "        # default to score-sde preprocessing\n",
    "        img = np.array(image).astype(np.uint8)\n",
    "\n",
    "        if self.center_crop:\n",
    "            crop = min(img.shape[0], img.shape[1])\n",
    "            (\n",
    "                h,\n",
    "                w,\n",
    "            ) = (\n",
    "                img.shape[0],\n",
    "                img.shape[1],\n",
    "            )\n",
    "            img = img[(h - crop) // 2 : (h + crop) // 2, (w - crop) // 2 : (w + crop) // 2]\n",
    "\n",
    "        image = Image.fromarray(img)\n",
    "        image = image.resize((self.size, self.size), resample=self.interpolation)\n",
    "\n",
    "        image = self.flip_transform(image)\n",
    "        image = np.array(image).astype(np.uint8)\n",
    "        image = (image / 127.5 - 1.0).astype(np.float32)\n",
    "\n",
    "        example[\"pixel_values\"] = torch.from_numpy(image).permute(2, 0, 1)\n",
    "        return example\n",
    "\n",
    "diffusers_dataset = TextualInversionDataset(\n",
    "    data_root=\"./my_data/H3D_samples\",\n",
    "    tokenizer=tokenizer,\n",
    "    size=256,\n",
    "    placeholder_token=(\" \".join(tokenizer.convert_ids_to_tokens(placeholder_token_ids))),\n",
    "    repeats=100,\n",
    "    learnable_property=\"style\",\n",
    "    center_crop=False,\n",
    "    set=\"train\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([49406,   320,  3638,  3086,   530,   518,  1844,   539,   283,   347,\n",
       "           324,   333,   323,   334,   325,   339,   324,   343,   339,   347,\n",
       "           285, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n",
       "         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n",
       "         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n",
       "         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n",
       "         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n",
       "         49407, 49407, 49407, 49407, 49407, 49407, 49407]),\n",
       " 'pixel_values': tensor([[[-0.8196, -0.8196, -0.8039,  ...,  0.1686, -0.3961, -0.4745],\n",
       "          [-0.8196, -0.8196, -0.8039,  ...,  0.1373, -0.4353, -0.4824],\n",
       "          [-0.8196, -0.8196, -0.8039,  ...,  0.0980, -0.4588, -0.4510],\n",
       "          ...,\n",
       "          [-0.9373, -0.9451, -0.9529,  ...,  0.9922,  0.8275,  0.4667],\n",
       "          [-0.9216, -0.9294, -0.9529,  ...,  0.9843,  0.8353,  0.4902],\n",
       "          [-0.9137, -0.9216, -0.9451,  ...,  0.9843,  0.8353,  0.4902]],\n",
       " \n",
       "         [[-0.8039, -0.8039, -0.7882,  ...,  0.2314, -0.3176, -0.3961],\n",
       "          [-0.8039, -0.8039, -0.7882,  ...,  0.1922, -0.3569, -0.4039],\n",
       "          [-0.8039, -0.8039, -0.7882,  ...,  0.1608, -0.3804, -0.3725],\n",
       "          ...,\n",
       "          [-0.9294, -0.9373, -0.9451,  ...,  1.0000,  0.8902,  0.5529],\n",
       "          [-0.9137, -0.9294, -0.9373,  ...,  1.0000,  0.8902,  0.5843],\n",
       "          [-0.9059, -0.9216, -0.9373,  ...,  1.0000,  0.8902,  0.5843]],\n",
       " \n",
       "         [[-0.7098, -0.7098, -0.6941,  ...,  0.1608, -0.4039, -0.4824],\n",
       "          [-0.7098, -0.7098, -0.6941,  ...,  0.1294, -0.4431, -0.4902],\n",
       "          [-0.7098, -0.7098, -0.6941,  ...,  0.0902, -0.4667, -0.4588],\n",
       "          ...,\n",
       "          [-0.8980, -0.9137, -0.9294,  ...,  0.9843,  0.8196,  0.4510],\n",
       "          [-0.8824, -0.9059, -0.9294,  ...,  0.9765,  0.8275,  0.4745],\n",
       "          [-0.8745, -0.8980, -0.9294,  ...,  0.9765,  0.8275,  0.4745]]])}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diffusers_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('./')\n",
    "import json\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "import pdb\n",
    "from annotator.util import resize_image, HWC3\n",
    "import random\n",
    "\n",
    "class UniDataset(Dataset):\n",
    "    def __init__(self, path_json, path_meta, task ):\n",
    "        self.data = []\n",
    "        with open(path_json, 'rt') as f:\n",
    "            for line in f:\n",
    "                self.data.append(json.loads(line))\n",
    "        self.path_meta = path_meta\n",
    "        if task == 'hed':\n",
    "            self.key_prompt = 'control_hed'\n",
    "        elif task == 'canny':\n",
    "            self.key_prompt = 'control_canny'\n",
    "        elif task == 'seg' or task == 'segbase':\n",
    "            self.key_prompt = 'control_seg'\n",
    "        elif task == 'depth':\n",
    "            self.key_prompt = 'control_depth'\n",
    "        elif task == 'normal':\n",
    "            self.key_prompt = 'control_normal'\n",
    "        elif task == 'openpose':\n",
    "            self.key_prompt = 'control_openpose'\n",
    "        elif task == 'hedsketch':\n",
    "            self.key_prompt = 'control_hedsketch'\n",
    "        elif task == 'bbox':\n",
    "            self.key_prompt = 'control_bbox'\n",
    "        elif task == 'outpainting':\n",
    "            self.key_prompt = 'control_outpainting' \n",
    "        elif task == 'inpainting':\n",
    "            self.key_prompt = 'control_inpainting'\n",
    "        elif task == 'blur':\n",
    "            self.key_prompt = 'control_blur'\n",
    "        elif task == 'grayscale':\n",
    "            self.key_prompt = 'control_grayscale'\n",
    "        else:\n",
    "            print('TASK NOT MATCH')\n",
    "            \n",
    "        self.resolution = 512\n",
    "        self.none_loop = 0\n",
    "        \n",
    "    def resize_image_control(self, control_image, resolution):\n",
    "        H, W, C = control_image.shape\n",
    "        if W >= H:\n",
    "            crop = H\n",
    "            crop_l = random.randint(0, W-crop) # 2nd value is inclusive\n",
    "            crop_r = crop_l + crop\n",
    "            crop_t = 0\n",
    "            crop_b = H\n",
    "        else:\n",
    "            crop = W\n",
    "            crop_t = random.randint(0, H-crop) # 2nd value is inclusive\n",
    "            crop_b = crop_t + crop\n",
    "            crop_l = 0\n",
    "            crop_r = W\n",
    "        control_image = control_image[ crop_t: crop_b, crop_l:crop_r]\n",
    "        H = float(H)\n",
    "        W = float(W)\n",
    "        k = float(resolution) / min(H, W)\n",
    "        img = cv2.resize(control_image, (resolution, resolution), interpolation=cv2.INTER_LANCZOS4 if k > 1 else cv2.INTER_AREA)\n",
    "        return img, [crop_t/H, crop_b/H, crop_l/W, crop_r/W]\n",
    "    \n",
    "    def resize_image_target(self, target_image, resolution, sizes):\n",
    "        H, W, C = target_image.shape\n",
    "        crop_t_rate, crop_b_rate, crop_l_rate, crop_r_rate = sizes[0], sizes[1], sizes[2], sizes[3]\n",
    "        crop_t, crop_b, crop_l, crop_r = int(crop_t_rate*H), int(crop_b_rate*H), int(crop_l_rate*W), int(crop_r_rate*W)\n",
    "        target_image = target_image[ crop_t: crop_b, crop_l:crop_r]\n",
    "        H = float(H)\n",
    "        W = float(W)\n",
    "        k = float(resolution) / min(H, W)\n",
    "        img = cv2.resize(target_image, (resolution, resolution), interpolation=cv2.INTER_LANCZOS4 if k > 1 else cv2.INTER_AREA)\n",
    "        return img\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        source_filename = item[self.key_prompt]\n",
    "        source_img = cv2.imread(self.path_meta + \"/conditions/\" + source_filename)\n",
    "        target_filename = item['source']\n",
    "        if \"./\" == target_filename[0:2]:\n",
    "            target_filename = target_filename[2:]\n",
    "        target_img = cv2.imread(self.path_meta+ \"/images/\" + target_filename)\n",
    "        prompt = item['prompt']\n",
    "        \n",
    "        while source_img is None or target_img is None or prompt is None:\n",
    "            # corner cases\n",
    "            if idx >= 0 and idx < len(self.data) - 1:\n",
    "                idx += 1\n",
    "            elif idx == len(self.data) - 1:\n",
    "                idx = 0\n",
    "            item = self.data[idx]\n",
    "            source_filename = item[self.key_prompt]\n",
    "            source_img = cv2.imread(self.path_meta + \"/conditions/\" + source_filename)\n",
    "            target_filename = item['source']\n",
    "            if \"./\" == target_filename[0:2]:\n",
    "                target_filename = target_filename[2:]\n",
    "            target_img = cv2.imread(self.path_meta+ \"/images/\" + target_filename)\n",
    "            prompt = item['prompt']\n",
    "            self.none_loop += 1\n",
    "            if self.none_loop > 10000:\n",
    "                break\n",
    "                \n",
    "        source_img,  sizes = self.resize_image_control(source_img, self.resolution)\n",
    "        target_img = self.resize_image_target(target_img, self.resolution, sizes)\n",
    "        \n",
    "        # Do not forget that OpenCV read images in BGR order.\n",
    "        source_img = cv2.cvtColor(source_img, cv2.COLOR_BGR2RGB)\n",
    "        target_img = cv2.cvtColor(target_img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Normalize source images to [0, 1].\n",
    "        source_img = source_img.astype(np.float32) / 255.0\n",
    "\n",
    "        # Normalize target images to [-1, 1].\n",
    "        target_img = (target_img.astype(np.float32) / 127.5) - 1.0\n",
    "        \n",
    "        prompt = prompt if random.uniform(0, 1) > 0.3 else '' # dropout rate 30%\n",
    "        return dict(jpg=target_img, txt=prompt, hint=source_img, task=self.key_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataset import ConcatDataset\n",
    "\n",
    "unicontrol_data = UniDataset(\"./research_dataset/json_files/roy.json\", \"./research_dataset\", \"grayscale\")\n",
    "unicontrol_data = ConcatDataset([unicontrol_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from train_util.multi_task_scheduler import BatchSchedulerSampler\n",
    "import train_util.dataset_collate as dataset_collate\n",
    "\n",
    "batch_size = 2\n",
    "dataloader = DataLoader(unicontrol_data, num_workers=16,  sampler=BatchSchedulerSampler(dataset=unicontrol_data, batch_size=batch_size), batch_size=batch_size, persistent_workers=True, shuffle=False, collate_fn=dataset_collate.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 512, 512, 3])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(dataloader))[\"jpg\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'jpg': array([[[-0.40392154, -0.32549018, -0.41176468],\n",
       "         [-0.5058824 , -0.42745095, -0.5137255 ],\n",
       "         [-0.4352941 , -0.35686272, -0.44313723],\n",
       "         ...,\n",
       "         [-0.81960785, -0.8039216 , -0.70980394],\n",
       "         [-0.81960785, -0.8039216 , -0.70980394],\n",
       "         [-0.81960785, -0.8039216 , -0.70980394]],\n",
       " \n",
       "        [[-0.41176468, -0.3333333 , -0.41960782],\n",
       "         [-0.52156866, -0.44313723, -0.5294118 ],\n",
       "         [-0.45098037, -0.372549  , -0.4588235 ],\n",
       "         ...,\n",
       "         [-0.81960785, -0.8039216 , -0.70980394],\n",
       "         [-0.81960785, -0.8039216 , -0.70980394],\n",
       "         [-0.81960785, -0.8039216 , -0.70980394]],\n",
       " \n",
       "        [[-0.41960782, -0.34117645, -0.42745095],\n",
       "         [-0.5294118 , -0.45098037, -0.5372549 ],\n",
       "         [-0.47450978, -0.3960784 , -0.4823529 ],\n",
       "         ...,\n",
       "         [-0.81960785, -0.8039216 , -0.70980394],\n",
       "         [-0.81960785, -0.8039216 , -0.70980394],\n",
       "         [-0.81960785, -0.8039216 , -0.70980394]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[ 0.38823533,  0.48235297,  0.37254906],\n",
       "         [ 0.56078434,  0.654902  ,  0.54509807],\n",
       "         [ 0.7882353 ,  0.84313726,  0.78039217],\n",
       "         ...,\n",
       "         [-0.92941177, -0.92156863, -0.90588236],\n",
       "         [-0.92156863, -0.9137255 , -0.88235295],\n",
       "         [-0.92156863, -0.9137255 , -0.88235295]],\n",
       " \n",
       "        [[ 0.38823533,  0.48235297,  0.37254906],\n",
       "         [ 0.56078434,  0.654902  ,  0.54509807],\n",
       "         [ 0.7882353 ,  0.84313726,  0.78039217],\n",
       "         ...,\n",
       "         [-0.92156863, -0.9137255 , -0.8980392 ],\n",
       "         [-0.9137255 , -0.90588236, -0.8745098 ],\n",
       "         [-0.9137255 , -0.90588236, -0.8745098 ]],\n",
       " \n",
       "        [[ 0.38823533,  0.48235297,  0.37254906],\n",
       "         [ 0.56078434,  0.654902  ,  0.54509807],\n",
       "         [ 0.7882353 ,  0.84313726,  0.78039217],\n",
       "         ...,\n",
       "         [-0.92156863, -0.9137255 , -0.8980392 ],\n",
       "         [-0.9137255 , -0.90588236, -0.8745098 ],\n",
       "         [-0.90588236, -0.8980392 , -0.8666667 ]]], dtype=float32),\n",
       " 'txt': 'test',\n",
       " 'hint': array([[[0.32156864, 0.32156864, 0.32156864],\n",
       "         [0.27058825, 0.27058825, 0.27058825],\n",
       "         [0.30588236, 0.30588236, 0.30588236],\n",
       "         ...,\n",
       "         [0.10196079, 0.10196079, 0.10196079],\n",
       "         [0.10196079, 0.10196079, 0.10196079],\n",
       "         [0.10196079, 0.10196079, 0.10196079]],\n",
       " \n",
       "        [[0.31764707, 0.31764707, 0.31764707],\n",
       "         [0.2627451 , 0.2627451 , 0.2627451 ],\n",
       "         [0.29803923, 0.29803923, 0.29803923],\n",
       "         ...,\n",
       "         [0.10196079, 0.10196079, 0.10196079],\n",
       "         [0.10196079, 0.10196079, 0.10196079],\n",
       "         [0.10196079, 0.10196079, 0.10196079]],\n",
       " \n",
       "        [[0.3137255 , 0.3137255 , 0.3137255 ],\n",
       "         [0.25882354, 0.25882354, 0.25882354],\n",
       "         [0.28627452, 0.28627452, 0.28627452],\n",
       "         ...,\n",
       "         [0.10196079, 0.10196079, 0.10196079],\n",
       "         [0.10196079, 0.10196079, 0.10196079],\n",
       "         [0.10196079, 0.10196079, 0.10196079]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[0.72156864, 0.72156864, 0.72156864],\n",
       "         [0.80784315, 0.80784315, 0.80784315],\n",
       "         [0.9098039 , 0.9098039 , 0.9098039 ],\n",
       "         ...,\n",
       "         [0.03921569, 0.03921569, 0.03921569],\n",
       "         [0.04313726, 0.04313726, 0.04313726],\n",
       "         [0.04313726, 0.04313726, 0.04313726]],\n",
       " \n",
       "        [[0.72156864, 0.72156864, 0.72156864],\n",
       "         [0.80784315, 0.80784315, 0.80784315],\n",
       "         [0.90588236, 0.90588236, 0.90588236],\n",
       "         ...,\n",
       "         [0.04313726, 0.04313726, 0.04313726],\n",
       "         [0.04705882, 0.04705882, 0.04705882],\n",
       "         [0.04705882, 0.04705882, 0.04705882]],\n",
       " \n",
       "        [[0.72156864, 0.72156864, 0.72156864],\n",
       "         [0.80784315, 0.80784315, 0.80784315],\n",
       "         [0.90588236, 0.90588236, 0.90588236],\n",
       "         ...,\n",
       "         [0.04313726, 0.04313726, 0.04313726],\n",
       "         [0.04705882, 0.04705882, 0.04705882],\n",
       "         [0.05098039, 0.05098039, 0.05098039]]], dtype=float32),\n",
       " 'task': 'control_grayscale'}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unicontrol_data[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unicontrol",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
