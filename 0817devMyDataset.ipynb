{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/disk/RDAIMT/roy_hsiao/NEW_ENV/venv/unicontrol/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-08-17 09:52:01.120890: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-08-17 09:52:01.241159: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ControlLDM: Running in eps-prediction mode\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 768 and using 8 heads.\n",
      "DiffusionWrapper has 859.52 M params.\n",
      "making attention of type 'vanilla-xformers' with 512 in_channels\n",
      "building MemoryEfficientAttnBlock with 512 in_channels...\n",
      "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
      "making attention of type 'vanilla-xformers' with 512 in_channels\n",
      "building MemoryEfficientAttnBlock with 512 in_channels...\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$ FrozenDict([('scaling_factor', 0.18215)])\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 768 and using 8 heads.\n",
      "Loaded model config from [./models/cldm_v15_unicontrol_v11.yaml]\n",
      "Loaded state_dict from [./ckpts/unicontrol_v1.1.ckpt]\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from packaging import version\n",
    "from torchvision import transforms\n",
    "import PIL\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "from PIL import Image\n",
    "from cldm.model import create_model, load_state_dict\n",
    "\n",
    "model = create_model(\"./models/cldm_v15_unicontrol_v11.yaml\").cpu()\n",
    "model.load_state_dict(load_state_dict(\"./ckpts/unicontrol_v1.1.ckpt\", location='cpu'), strict=False) #, strict=False\n",
    "tokenizer = model.cond_stage_model.tokenizer\n",
    "placeholder_tokens = \"<target-hand>\"\n",
    "placeholder_token_ids = tokenizer.convert_tokens_to_ids(placeholder_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "if version.parse(version.parse(PIL.__version__).base_version) >= version.parse(\"9.1.0\"):\n",
    "    PIL_INTERPOLATION = {\n",
    "        \"linear\": PIL.Image.Resampling.BILINEAR,\n",
    "        \"bilinear\": PIL.Image.Resampling.BILINEAR,\n",
    "        \"bicubic\": PIL.Image.Resampling.BICUBIC,\n",
    "        \"lanczos\": PIL.Image.Resampling.LANCZOS,\n",
    "        \"nearest\": PIL.Image.Resampling.NEAREST,\n",
    "    }\n",
    "else:\n",
    "    PIL_INTERPOLATION = {\n",
    "        \"linear\": PIL.Image.LINEAR,\n",
    "        \"bilinear\": PIL.Image.BILINEAR,\n",
    "        \"bicubic\": PIL.Image.BICUBIC,\n",
    "        \"lanczos\": PIL.Image.LANCZOS,\n",
    "        \"nearest\": PIL.Image.NEAREST,\n",
    "    }\n",
    "    \n",
    "imagenet_templates_small = [\n",
    "    \"a photo of a {}\",\n",
    "    \"a rendering of a {}\",\n",
    "    \"a cropped photo of the {}\",\n",
    "    \"the photo of a {}\",\n",
    "    \"a photo of a clean {}\",\n",
    "    \"a photo of a dirty {}\",\n",
    "    \"a dark photo of the {}\",\n",
    "    \"a photo of my {}\",\n",
    "    \"a photo of the cool {}\",\n",
    "    \"a close-up photo of a {}\",\n",
    "    \"a bright photo of the {}\",\n",
    "    \"a cropped photo of a {}\",\n",
    "    \"a photo of the {}\",\n",
    "    \"a good photo of the {}\",\n",
    "    \"a photo of one {}\",\n",
    "    \"a close-up photo of the {}\",\n",
    "    \"a rendition of the {}\",\n",
    "    \"a photo of the clean {}\",\n",
    "    \"a rendition of a {}\",\n",
    "    \"a photo of a nice {}\",\n",
    "    \"a good photo of a {}\",\n",
    "    \"a photo of the nice {}\",\n",
    "    \"a photo of the small {}\",\n",
    "    \"a photo of the weird {}\",\n",
    "    \"a photo of the large {}\",\n",
    "    \"a photo of a cool {}\",\n",
    "    \"a photo of a small {}\",\n",
    "]\n",
    "\n",
    "imagenet_style_templates_small = [\n",
    "    \"a painting in the style of {}\",\n",
    "    \"a rendering in the style of {}\",\n",
    "    \"a cropped painting in the style of {}\",\n",
    "    \"the painting in the style of {}\",\n",
    "    \"a clean painting in the style of {}\",\n",
    "    \"a dirty painting in the style of {}\",\n",
    "    \"a dark painting in the style of {}\",\n",
    "    \"a picture in the style of {}\",\n",
    "    \"a cool painting in the style of {}\",\n",
    "    \"a close-up painting in the style of {}\",\n",
    "    \"a bright painting in the style of {}\",\n",
    "    \"a cropped painting in the style of {}\",\n",
    "    \"a good painting in the style of {}\",\n",
    "    \"a close-up painting in the style of {}\",\n",
    "    \"a rendition in the style of {}\",\n",
    "    \"a nice painting in the style of {}\",\n",
    "    \"a small painting in the style of {}\",\n",
    "    \"a weird painting in the style of {}\",\n",
    "    \"a large painting in the style of {}\",\n",
    "]\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_root,\n",
    "        tokenizer,\n",
    "        learnable_property=\"object\",  # [object, style]\n",
    "        size=512,\n",
    "        repeats=100,\n",
    "        interpolation=\"bicubic\",\n",
    "        flip_p=0.5,\n",
    "        set=\"train\",\n",
    "        placeholder_token=\"*\",\n",
    "        center_crop=False,\n",
    "    ):\n",
    "        self.data_root = data_root\n",
    "        self.tokenizer = tokenizer\n",
    "        self.learnable_property = learnable_property\n",
    "        self.size = size\n",
    "        self.placeholder_token = placeholder_token\n",
    "        self.center_crop = center_crop\n",
    "        self.flip_p = flip_p\n",
    "\n",
    "        self.image_paths = [os.path.join(self.data_root, file_path) for file_path in os.listdir(self.data_root)]\n",
    "\n",
    "        self.num_images = len(self.image_paths)\n",
    "        self._length = self.num_images\n",
    "\n",
    "        if set == \"train\":\n",
    "            self._length = self.num_images * repeats\n",
    "\n",
    "        self.interpolation = {\n",
    "            \"linear\": PIL_INTERPOLATION[\"linear\"],\n",
    "            \"bilinear\": PIL_INTERPOLATION[\"bilinear\"],\n",
    "            \"bicubic\": PIL_INTERPOLATION[\"bicubic\"],\n",
    "            \"lanczos\": PIL_INTERPOLATION[\"lanczos\"],\n",
    "        }[interpolation]\n",
    "\n",
    "        self.templates = imagenet_style_templates_small if learnable_property == \"style\" else imagenet_templates_small\n",
    "        self.flip_transform = transforms.RandomHorizontalFlip(p=self.flip_p)\n",
    "\n",
    "    def resize_image_control(self, control_image, resolution):\n",
    "        H, W, C = control_image.shape\n",
    "        if W >= H:\n",
    "            crop = H\n",
    "            crop_l = random.randint(0, W-crop) # 2nd value is inclusive\n",
    "            crop_r = crop_l + crop\n",
    "            crop_t = 0\n",
    "            crop_b = H\n",
    "        else:\n",
    "            crop = W\n",
    "            crop_t = random.randint(0, H-crop) # 2nd value is inclusive\n",
    "            crop_b = crop_t + crop\n",
    "            crop_l = 0\n",
    "            crop_r = W\n",
    "        control_image = control_image[ crop_t: crop_b, crop_l:crop_r]\n",
    "        H = float(H)\n",
    "        W = float(W)\n",
    "        k = float(resolution) / min(H, W)\n",
    "        img = cv2.resize(control_image, (resolution, resolution), interpolation=cv2.INTER_LANCZOS4 if k > 1 else cv2.INTER_AREA)\n",
    "        return img, [crop_t/H, crop_b/H, crop_l/W, crop_r/W]\n",
    "    \n",
    "    def resize_image_target(self, target_image, resolution, sizes):\n",
    "        H, W, C = target_image.shape\n",
    "        crop_t_rate, crop_b_rate, crop_l_rate, crop_r_rate = sizes[0], sizes[1], sizes[2], sizes[3]\n",
    "        crop_t, crop_b, crop_l, crop_r = int(crop_t_rate*H), int(crop_b_rate*H), int(crop_l_rate*W), int(crop_r_rate*W)\n",
    "        target_image = target_image[ crop_t: crop_b, crop_l:crop_r]\n",
    "        H = float(H)\n",
    "        W = float(W)\n",
    "        k = float(resolution) / min(H, W)\n",
    "        img = cv2.resize(target_image, (resolution, resolution), interpolation=cv2.INTER_LANCZOS4 if k > 1 else cv2.INTER_AREA)\n",
    "        return img\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self._length\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        example = {}\n",
    "        source_img = cv2.imread(self.image_paths[i % self.num_images],cv2.IMREAD_GRAYSCALE)\n",
    "        target_img = cv2.imread(self.image_paths[i % self.num_images])\n",
    "\n",
    "        placeholder_string = self.placeholder_token\n",
    "        text = random.choice(self.templates).format(placeholder_string)\n",
    "\n",
    "        prompt = self.tokenizer(\n",
    "            text,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.tokenizer.model_max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        ).input_ids[0]\n",
    "\n",
    "        source_img,  sizes = self.resize_image_control(source_img, self.resolution)\n",
    "        target_img = self.resize_image_target(target_img, self.resolution, sizes)\n",
    "\n",
    "        # Do not forget that OpenCV read images in BGR order.\n",
    "        source_img = cv2.cvtColor(source_img, cv2.COLOR_BGR2RGB)\n",
    "        target_img = cv2.cvtColor(target_img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Normalize source images to [0, 1].\n",
    "        source_img = source_img.astype(np.float32) / 255.0\n",
    "\n",
    "        # Normalize target images to [-1, 1].\n",
    "        target_img = (target_img.astype(np.float32) / 127.5) - 1.0\n",
    "        \n",
    "        return dict(jpg=target_img, txt=prompt, hint=source_img, task=\"control_grayscale\")\n",
    "\n",
    "diffusers_dataset = MyDataset(\n",
    "    data_root=\"./my_data/H3D_samples\",\n",
    "    tokenizer=tokenizer,\n",
    "    size=256,\n",
    "    placeholder_token=(\" \".join(tokenizer.convert_ids_to_tokens(placeholder_token_ids))),\n",
    "    repeats=100,\n",
    "    learnable_property=\"style\",\n",
    "    center_crop=False,\n",
    "    set=\"train\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'MyDataset' object has no attribute 'resize_image_control'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m diffusers_dataset[\u001b[39m0\u001b[39;49m]\n",
      "Cell \u001b[0;32mIn[8], line 142\u001b[0m, in \u001b[0;36mMyDataset.__getitem__\u001b[0;34m(self, i)\u001b[0m\n\u001b[1;32m    132\u001b[0m text \u001b[39m=\u001b[39m random\u001b[39m.\u001b[39mchoice(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtemplates)\u001b[39m.\u001b[39mformat(placeholder_string)\n\u001b[1;32m    134\u001b[0m prompt \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer(\n\u001b[1;32m    135\u001b[0m     text,\n\u001b[1;32m    136\u001b[0m     padding\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmax_length\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    139\u001b[0m     return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    140\u001b[0m )\u001b[39m.\u001b[39minput_ids[\u001b[39m0\u001b[39m]\n\u001b[0;32m--> 142\u001b[0m source_img,  sizes \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mresize_image_control(source_img, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresolution)\n\u001b[1;32m    143\u001b[0m target_img \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresize_image_target(target_img, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresolution, sizes)\n\u001b[1;32m    145\u001b[0m \u001b[39m# Do not forget that OpenCV read images in BGR order.\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'MyDataset' object has no attribute 'resize_image_control'"
     ]
    }
   ],
   "source": [
    "diffusers_dataset[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unicontrol",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
